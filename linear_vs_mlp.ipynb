{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_vs_mlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8S2dWLa8gH9BURbMK5nZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyrankhademi/introduction2AI/blob/main/linear_vs_mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFKku8LTF6DC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6PtWKtnm0B8"
      },
      "source": [
        "# Computer Programming vs Machine Learning \n",
        "This notebook is written by Dr. Seyran Khademi to familiarize the students with the concept of machine learning and its difference with computer programming. The code is developed in Jupyter notebook and it is compatible with the Google Colab platform. \n",
        "\n",
        "---\n",
        "## What is fundamentally different between machine learning and computer programming?\n",
        "\n",
        "Let's look at an example project. Suppose that university wants to automate the process of granting scholarship to the students.  They need a computer to make a decision whether an applicant is eligible for the scholarship or not based on some handcrafted features extracted by people at university including 1) grade point average (GPA) 2) quality of portfolio(QP) 3) Age and 4) whether the applicant has other loans or not. So for each applicant, there is a given tabular data like the following: \n",
        "\n",
        "| Features | Applicant   \n",
        "|------|------|\n",
        "|   GPA  | a number between [0,10]|\n",
        "|   QP  |a number between [0,10] |\n",
        "|   Age  |an integer between [18,40]|\n",
        "|   Loan  |1 or 0 for loan/no loan|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie40kIARwm3r"
      },
      "source": [
        "Weighted-sum program\n",
        "In the first attempt, we use a piece of code that computes the weighted sum of the given features as the final score for the applicant. In computer programming, the program (the rules) are set by the human explicitly. In our scholarship project, the rules are the weights for each feature,i.e., $[w_1,w_2,w_3,w_4]$. Note that each weight is the importance of the corresponding feature in the final score. Suppose that the committee for the scholarship assignment proposes the following weights $[0.4.0.3,0.2,0.1]$  respectively. The following cell is the code snippet that computes the final score of the applicant by the given weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvVqOU7eWi3N"
      },
      "source": [
        "# The weighet-sum function takes as an input the feature values for the applicant\n",
        "# and outputs the final score. \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def weighted_sum(GPA,QP,Age,Loan):\n",
        "  #check that the points for GPA and QP are in range between 0 and 10\n",
        "  x=GPA\n",
        "  y=QP\n",
        "  points = np.array([x,y])\n",
        "  if (points < 0).all() and (points > 10).all(): \n",
        "    print(\"Error: The GPA and PQ points must be between 0 and 10.\")\n",
        "\n",
        "  #check that the age in range between 18 and 40\n",
        "  z=Age\n",
        "  if (z < 18) or (z > 40): \n",
        "    print(\"Note: Applicants younger than 18 and older than 40 are not eligible for the scholorship.\")\n",
        "\n",
        "  #check that the loan feature is specified as binary \n",
        "  v=Loan\n",
        "  if (z ==0 or z==1): \n",
        "    print(\"Error: If the applicant has other loans currently enter 1 otherwise enter 0 for the Loan feature.\")   \n",
        "\n",
        "  #compute the weighed sum score\n",
        "  w1=0.4\n",
        "  w2=0.3\n",
        "  w3=0.2\n",
        "  w4=0.1\n",
        "  Score=w1*x+w2*y+w3*z+w4*v\n",
        "  print(\"Final score for the applicant is\", + Score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7beQ0wH0CD5"
      },
      "source": [
        "Let's see what is the score for Sara given the folowing records: \n",
        "\n",
        "| Features | Sara  \n",
        "|------|------|\n",
        "|   GPA  | 7.8|\n",
        "|   QP  |6.5 |\n",
        "|   Age  |26|\n",
        "|   Loan  |0|\n",
        "\n",
        "We call the function ```weighted_sum``` to compute the score ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETB3Wytz1CdV"
      },
      "source": [
        "weighted_sum(7.8,6.5,26,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEt3erEo1nuc"
      },
      "source": [
        "By running the code cell you found out what is the final score for Sara. In case the scholarship is competitive you need to compute the scores for other applicants and see whether Sara is in, e.g., the top 50% or not but we stop here as we made already our point. The weights (the selection rule) are given to the computer for this task by the human experts that learned the weightings empirically over the years of doing their job!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV__jfzN2yHa"
      },
      "source": [
        "## Machine Learning (ML)\n",
        "\n",
        "University collected enough digital records from the students who have applied for the scholarship together with the outcome based on the following criteria that Whether the student \n",
        " \n",
        "\n",
        "1.   finished master studies in less than two year\n",
        "2.   has returned the loan within 10 years\n",
        "\n",
        "Given the amount of data the university decided to replace the averaging software with an ML  model that can be *trained* on the available data to *learn* the selection rules. \n",
        "\n",
        "---\n",
        "In the following code cell, we generate some synthetic data for this task as we don't really have the students record. For the sake of visualization purposes, we only take two features per applicant lets to say GPA and QP. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEdghDOD6MDY"
      },
      "source": [
        "# generate syntatic data with two features (GPA and QP) and two class labels (sucsesful or not) \n",
        "from sklearn.datasets import make_moons, make_circles, make_classification,make_gaussian_quantiles\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "# The random-state in the data generator is set to fix for reproducibility. \n",
        "data,labels =  make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, class_sep=0.9, random_state=42)\n",
        "# fit the features in the range [0 10]\n",
        "scaler = MinMaxScaler(feature_range=(0, 10))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "# plot samples of data points with their labels \n",
        "plt.scatter(data_scaled [:, 0], data_scaled [:, 1], marker='o', c=labels, s=100, edgecolor='k')\n",
        "plt.xlabel('Quality of portfolio')\n",
        "plt.ylabel('GPA')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHwiaG6FSAQA"
      },
      "source": [
        "So you should see a figure with two classes \"Purple\" and \"Yellow\". Can you guess which class represents the \"successful\" students?\n",
        "Next we train a simple ML model on these data to classify \"Purple\" from \"Yellow\". \n",
        "\n",
        "---\n",
        "We need to split our data into the test and train sets. The test set is used for evaluation of the model and the training set is for the model to learn the best decision-making rule from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV9fSDfPNCqX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " # normalizing data to get best performance \n",
        "X = StandardScaler().fit_transform(data)\n",
        " # splitting data to train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=.4, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfnbi3Y2rW1G"
      },
      "source": [
        "from sklearn import metrics\n",
        "# evalution function takes the test data and the clf and calculates the accuracy of the model\n",
        "def evaluate(X_test,clf):\n",
        " y_pred = clf.predict(X_test)\n",
        " print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRkJjHgs5WIh"
      },
      "source": [
        "Our first model is a simple linear classifer to be trained and evaluted on the train and the test data respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3aG1cIsCwFu"
      },
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train,y_train)\n",
        "evaluate(X_test,clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx3mEbBp7KZC"
      },
      "source": [
        "Our simple ML model performs with $86\\%$ accuracy. Is that acceptable for our application? \n",
        "\n",
        "---\n",
        "\n",
        "Let's look closer to the data again and the decision boundry of our trained classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdL0QXXWgtAW"
      },
      "source": [
        "# the function gets the training data, labels and the classifier and plots decision boundry \n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.gridspec as gridspec\n",
        "def plot_decision_boundary(X_train,y_train,clf):\n",
        " gs = gridspec.GridSpec(2, 2)\n",
        " fig = plot_decision_regions(X_train, y_train.astype(np.integer),clf=clf, legend=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWcvGnmm9NiT"
      },
      "source": [
        "plot_decision_boundary(X_train,y_train,clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO6d5hLA9itI"
      },
      "source": [
        "As you can see the linear classifier called support vector machine (SVM) may not be flexible enough to separate our (normalized) data. \n",
        "\n",
        "## Neural Network\n",
        "The next classifier that we try is a very simple Neural Network that is a very basic nonlinear model that is called a multi-layer perceptron (MLP). \n",
        "MLP is more flexible than our simple SVM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypt6jiXU5qyD"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='adam', alpha=1e-3, hidden_layer_sizes=(5, 2), learning_rate_init=0.005, max_iter=1000, random_state=1)\n",
        "clf.fit(X_train,y_train)\n",
        "evaluate(X_test,clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3h8NHi_TGo"
      },
      "source": [
        "Our accuracy improved to $90\\%$ using the MLP. Let's look at the decision plot to get a glance at our neural network classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dMjHmvbOZel"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHYBdlUE_4C1"
      },
      "source": [
        "plot_decision_boundary(X_train,y_train,clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Z8AuGJPzlp"
      },
      "source": [
        "It is clear that our MLP model with more parameters (thus more complex) can adapt better to our synthetic dataset. In deep learning, the computer model is much more complex with thousands and millions of parameters to adapt to data with a complex nature. Nevertheless, training deep learning models requires great computational power and training time so we skip it in this tutorial. For real data and proper deep learning models, you can visit https://github.com/seyrankhademi/ResNet_CIFAR10. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L3IyoxYMpIl"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Once we have complex data, we need complex models to analyze that. Our synthetic data is way more simple in terms of dimensionality compared to real data captured from our world. However, we observed the effect of introducing a relatively more complex model compared to the linear model for the task of classification even in this simplified setting. Note that deep learning follows the same rules of statistical learning as developed for years in machine learning, however, we had not had enough computational power up to just recent years and such a huge amount of data to process in order to deploy deep learning. "
      ]
    }
  ]
}